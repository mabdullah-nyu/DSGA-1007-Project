{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef7d0dbe",
   "metadata": {},
   "source": [
    "Yelp API Guide: https://docs.developer.yelp.com/docs/fusion-intro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dffff413",
   "metadata": {},
   "source": [
    "## Aggregating Yelp Restaurant Data\n",
    "Uses the v3/businesses/search endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b037cbc0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-13T03:34:41.623893Z",
     "start_time": "2023-11-13T03:34:41.618320Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import requests\n",
    "import time\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d4efaae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-11T00:33:59.773074Z",
     "start_time": "2023-11-11T00:33:59.761205Z"
    }
   },
   "outputs": [],
   "source": [
    "# Aggregate Manhattan specific restuarants by neighborhood for more granular results.\n",
    "neighborhoods = [\n",
    "    \"Alphabet City\",\n",
    "    \"Battery Park City\",\n",
    "    \"Bowery\",\n",
    "    \"Bryant Park\",\n",
    "    \"Carnegie Hill\",\n",
    "    \"Central Park\",\n",
    "    \"Chelsea\",\n",
    "    \"Chinatown\",\n",
    "    \"Civic Center\",\n",
    "    \"Clinton\",\n",
    "    \"East Harlem\",\n",
    "    \"East Village\",\n",
    "    \"Financial District\",\n",
    "    \"Flatiron\",\n",
    "    \"Fort George\",\n",
    "    \"Garment District\",\n",
    "    \"Gramercy\",\n",
    "    \"Greenwich Village\",\n",
    "    \"Hamilton Heights\",\n",
    "    \"Harlem\",\n",
    "    \"Hells Kitchen\",\n",
    "    \"Hudson Heights\",\n",
    "    \"Hudson Square\",\n",
    "    \"Hudson Yards\",\n",
    "    \"Inwood\",\n",
    "    \"Kips Bay\",\n",
    "    \"Lenox Hill\",\n",
    "    \"Lincoln Square\",\n",
    "    \"Little Italy\",\n",
    "    \"Lower East Side\",\n",
    "    \"Manhattan Valley\",\n",
    "    \"Manhattanville\",\n",
    "    \"Meatpacking\",\n",
    "    \"Midtown\",\n",
    "    \"Midtown East\",\n",
    "    \"Midtown South\",\n",
    "    \"Midtown West\",\n",
    "    \"Morningside Heights\",\n",
    "    \"Murray Hill\",\n",
    "    \"Noho\",\n",
    "    \"Nolita\",\n",
    "    \"NoMad\",\n",
    "    \"Roosevelt Island\",\n",
    "    \"Soho\",\n",
    "    \"Stuyvesant Town\",\n",
    "    \"Sutton Place\",\n",
    "    \"Times Square\",\n",
    "    \"Theater District\",\n",
    "    \"Tribeca\",\n",
    "    \"Tudor City\",\n",
    "    \"Turtle Bay\",\n",
    "    \"Two Bridges\",\n",
    "    \"Union Square\",\n",
    "    \"Upper East Side\",\n",
    "    \"Upper West Side\",\n",
    "    \"Washington Heights\",\n",
    "    \"Washington Square Park\",\n",
    "    \"West Harlem\",\n",
    "    \"West Village\",\n",
    "    \"Yorkville\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24e34197",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-11T00:01:04.749938Z",
     "start_time": "2023-11-11T00:01:04.744671Z"
    }
   },
   "outputs": [],
   "source": [
    "# Yelp allows inly 500 API calls per day, so we rotate btwn 3 different keys for maximum data collection.\n",
    "kKeyIndex = 0\n",
    "keys = [\n",
    "    \"ZF5VOfPCUWtK2C4_ZMpMrO3FxyS6EGlN_aCjNPBTYZyHhmMZvi7sADCFioEuDUalKlL_83AGB1fWkICmFeHudLzmUhtUq589kgKpnfQbQoT2BMznqTLJ2cIX1RRAZXYx\",\n",
    "    \"QOCKsANBYQUN4Fmrxh23mAl5Bjbi69gv3W7ChGNOmp98Q3124aytz9F2MzEPhmKOXa6EomrQAjLeGEZuvlrbsR5Q_KSnsST7Ona_K0_wafErqsrxsd68aCSe9j9IZXYx\",\n",
    "    \"NO9vZwZGnE58R8YbQDEPC90SlZ2eok4O4aYkdIxH96vUZMeSCDCvIZYY7L3VxWVYiMITiaMIkOBPRdtOgkR52BwBexnpVDDmhcjWClFRgu8uByoBopPAP8stZUBIZXYx\"  \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa88279f",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://api.yelp.com/v3/businesses/search'\n",
    "headers = {\n",
    "    \"accept\": \"application/json\",\n",
    "    \"Authorization\": \"Bearer \" + keys[kKeyIndex]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd6b5355",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-11T00:07:17.247366Z",
     "start_time": "2023-11-11T00:07:17.240376Z"
    }
   },
   "outputs": [],
   "source": [
    "alias_to_content = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6deb81c6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-11T00:35:24.727570Z",
     "start_time": "2023-11-11T00:34:09.577775Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching data for Alphabet City\n",
      "Fetching data for Battery Park City\n",
      "Fetching data for Bowery\n",
      "Fetching data for Bryant Park\n",
      "Fetching data for Carnegie Hill\n",
      "Fetching data for Central Park\n",
      "Fetching data for Chelsea\n",
      "Fetching data for Chinatown\n",
      "Fetching data for Civic Center\n",
      "Fetching data for Clinton\n",
      "Fetching data for East Harlem\n",
      "Fetching data for East Village\n",
      "Fetching data for Financial District\n",
      "Fetching data for Flatiron\n",
      "Fetching data for Fort George\n",
      "Fetching data for Garment District\n",
      "Fetching data for Gramercy\n",
      "Fetching data for Greenwich Village\n",
      "Fetching data for Hamilton Heights\n",
      "Fetching data for Harlem\n",
      "Fetching data for Hells Kitchen\n",
      "Fetching data for Hudson Heights\n",
      "Fetching data for Hudson Square\n",
      "Fetching data for Hudson Yards\n",
      "Fetching data for Inwood\n",
      "Fetching data for Kips Bay\n",
      "Fetching data for Lenox Hill\n",
      "Fetching data for Lincoln Square\n",
      "Fetching data for Little Italy\n",
      "Fetching data for Lower East Side\n",
      "Fetching data for Manhattan Valley\n",
      "Fetching data for Manhattanville\n",
      "Rotating key\n",
      "Fetching data for Meatpacking\n",
      "Fetching data for Midtown\n",
      "Fetching data for Midtown East\n",
      "Fetching data for Midtown South\n",
      "Fetching data for Midtown West\n",
      "Fetching data for Morningside Heights\n",
      "Fetching data for Murray Hill\n",
      "Fetching data for Noho\n",
      "Fetching data for Nolita\n",
      "Fetching data for NoMad\n",
      "Fetching data for Roosevelt Island\n",
      "Fetching data for Soho\n",
      "Fetching data for Stuyvesant Town\n",
      "Fetching data for Sutton Place\n",
      "Fetching data for Times Square\n",
      "Fetching data for Theater District\n",
      "Fetching data for Tribeca\n",
      "Rotating key\n",
      "Fetching data for Tudor City\n",
      "Fetching data for Turtle Bay\n",
      "Fetching data for Two Bridges\n",
      "Fetching data for Union Square\n",
      "Fetching data for Upper East Side\n",
      "Fetching data for Upper West Side\n",
      "Fetching data for Washington Heights\n",
      "Fetching data for Washington Square Park\n",
      "Fetching data for West Harlem\n",
      "Fetching data for West Village\n",
      "Fetching data for Yorkville\n"
     ]
    }
   ],
   "source": [
    "for neighborhood in neighborhoods:\n",
    "    print(\"Fetching data for \" + neighborhood)\n",
    "    \n",
    "    # Maximum results per API request.\n",
    "    limit = 50\n",
    "    location = neighborhood + \", Manhattan, NY\"\n",
    "    location = location.replace(\" \", \"+\")\n",
    "    \n",
    "    # Get up to 1000 restaurants per neighborhood.\n",
    "    for i in range(0, 1000, limit):        \n",
    "        url_params = {\n",
    "            \"location\": location,\n",
    "            \"term\": \"Restaurants\",\n",
    "            \"limit\": limit,\n",
    "            \"offset\": i,\n",
    "            \"categories\": \"(restaurants, All)\",\n",
    "            \"sort_by\": \"distance\",\n",
    "        }\n",
    "\n",
    "        response = requests.get(url, headers=headers, params=url_params)\n",
    "        \n",
    "        # Max API calls gets a return status == 429!\n",
    "        if response.status_code == 429: \n",
    "            print(\"Rotating key\")\n",
    "            kKeyIndex += 1\n",
    "            headers[\"Authorization\"] = \"Bearer \" + keys[kKeyIndex]\n",
    "            response = requests.get(url, headers=headers, params=url_params)\n",
    "\n",
    "        if response.status_code != 200:\n",
    "            print(response.status_code)\n",
    "            continue\n",
    "\n",
    "        # If we already got all the businesses in a neighborhood.\n",
    "        content = json.loads(response.content)\n",
    "        if len(content[\"businesses\"]) == 0:\n",
    "            break\n",
    "\n",
    "        for business in content[\"businesses\"]:\n",
    "            alias_to_content[business[\"alias\"]] = business"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e427d0a4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-11T00:36:22.076014Z",
     "start_time": "2023-11-11T00:36:21.539270Z"
    }
   },
   "outputs": [],
   "source": [
    "# Write file.\n",
    "#file_path = \"{}/restaurants.json\".format(os.getcwd())\n",
    "#with open(file_path, \"w\") as fp:\n",
    "#    json.dump(alias_to_content, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "571bd46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy file\n",
    "file_path = \"{}/restaurants2.json\".format(os.getcwd())\n",
    "with open(file_path, \"w\") as fp:\n",
    "    json.dump(alias_to_content, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153f5fb8",
   "metadata": {},
   "source": [
    "## Aggregating Yelp Reviews Data\n",
    "No API for this. We call it with a sketchy endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "32eef56c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-13T03:55:15.042518Z",
     "start_time": "2023-11-13T03:55:14.703211Z"
    }
   },
   "outputs": [],
   "source": [
    "# Read file.\n",
    "#file_path = \"{}/restaurants.json\".format(os.getcwd())\n",
    "#with open(file_path, \"r\") as json_file:\n",
    "#    alias_to_content = json.loads(json_file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cad813f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy\n",
    "file_path = \"{}/restaurants2.json\".format(os.getcwd())\n",
    "with open(file_path, \"r\") as json_file:\n",
    "    alias_to_content = json.loads(json_file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d0c194a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-13T03:55:16.471657Z",
     "start_time": "2023-11-13T03:55:16.466897Z"
    }
   },
   "outputs": [],
   "source": [
    "headers = {\n",
    "    \"Content-Type\": \"text/html; charset=UTF-8\",\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a0af6ecb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-13T03:55:19.565919Z",
     "start_time": "2023-11-13T03:55:17.472185Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching for thirteen-water-new-york\n",
      "Fetching for sake-bar-satsko-new-york\n",
      "Fetching for tokuyamatcha-and-onigirazu-bar-new-york-3\n",
      "Fetching for the-frenchy-burger-new-york\n",
      "Fetching for pmf-pardon-my-french-new-york\n",
      "Fetching for san-loco-new-york-5\n",
      "Fetching for ayat-new-york\n",
      "Fetching for thayer-new-york\n",
      "Fetching for two-perrys-new-york\n",
      "Fetching for your-desire-in-food-new-york-2\n",
      "Fetching for bar-miller-new-york\n",
      "Fetching for horus-cafe-new-york-5\n",
      "Fetching for the-summit-bar-new-york\n",
      "Fetching for puerto-rico-ices-new-york\n",
      "Fetching for kafana-new-york\n",
      "Fetching for ding-a-ling-new-york\n",
      "Fetching for esqueleto-taqueria-new-york-3\n",
      "Fetching for maia-meyhane-new-york\n",
      "Fetching for gruppo-nycthincrust-pizza-new-york\n",
      "Fetching for yankee-two-deli-manhattan\n",
      "Fetching for c-and-b-new-york\n",
      "Fetching for sunny-and-annies-new-york\n",
      "Fetching for bobwhite-counter-new-york-7\n",
      "Fetching for lis-restaurant-new-york\n",
      "Fetching for a-and-c-kitchen-new-york-2\n",
      "Fetching for lilys-shakes-and-crepes-new-york\n",
      "Fetching for bobby-s-night-out-new-york\n",
      "Fetching for n-y-famous-pizzeria-new-york\n",
      "Fetching for the-wayland-new-york\n",
      "Fetching for studio-151-new-york\n",
      "Fetching for soda-club-new-york\n",
      "Fetching for gustavo-new-york\n",
      "Fetching for anthonys-do-it-like-nyc-new-york\n",
      "Fetching for veg-n-new-york\n",
      "Fetching for tajeen-halal-food-truck-new-york-2\n",
      "Fetching for jaleby-new-york-2\n",
      "Fetching for tyga-bites-new-york-3\n",
      "Fetching for spicy-street-new-york\n",
      "Fetching for royale-new-york\n",
      "Fetching for neighborhood-deli-new-york-2\n",
      "Fetching for imperio-food-grocery-new-york\n",
      "Fetching for lavagna-new-york\n",
      "Fetching for greenland-gourmet-deli-new-york\n",
      "Fetching for maiden-lane-new-york\n",
      "Fetching for 5c-cafe-new-york\n",
      "Fetching for casa-adela-new-york\n",
      "Fetching for pizza-nostra-new-york\n",
      "Fetching for gnocco-new-york\n",
      "Fetching for hekate-caf√©-and-elixir-lounge-new-york\n"
     ]
    },
    {
     "ename": "UnicodeEncodeError",
     "evalue": "'ascii' codec can't encode character '\\xe9' in position 19: ordinal not in range(128)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeEncodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m/Users/willcalandra/Documents/DSGA-1007-Project/scrape.ipynb Cell 15\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/willcalandra/Documents/DSGA-1007-Project/scrape.ipynb#X15sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m# Sleep or else Yelp might block your IP.\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/willcalandra/Documents/DSGA-1007-Project/scrape.ipynb#X15sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m time\u001b[39m.\u001b[39msleep(\u001b[39m2\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/willcalandra/Documents/DSGA-1007-Project/scrape.ipynb#X15sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39mwith\u001b[39;00m urllib\u001b[39m.\u001b[39mrequest\u001b[39m.\u001b[39murlopen(request) \u001b[39mas\u001b[39;00m response:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/willcalandra/Documents/DSGA-1007-Project/scrape.ipynb#X15sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     \u001b[39mif\u001b[39;00m response\u001b[39m.\u001b[39mcode \u001b[39m!=\u001b[39m \u001b[39m200\u001b[39m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/willcalandra/Documents/DSGA-1007-Project/scrape.ipynb#X15sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m         \u001b[39mprint\u001b[39m(response\u001b[39m.\u001b[39mcode)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/urllib/request.py:216\u001b[0m, in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    215\u001b[0m     opener \u001b[39m=\u001b[39m _opener\n\u001b[0;32m--> 216\u001b[0m \u001b[39mreturn\u001b[39;00m opener\u001b[39m.\u001b[39mopen(url, data, timeout)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/urllib/request.py:519\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    516\u001b[0m     req \u001b[39m=\u001b[39m meth(req)\n\u001b[1;32m    518\u001b[0m sys\u001b[39m.\u001b[39maudit(\u001b[39m'\u001b[39m\u001b[39murllib.Request\u001b[39m\u001b[39m'\u001b[39m, req\u001b[39m.\u001b[39mfull_url, req\u001b[39m.\u001b[39mdata, req\u001b[39m.\u001b[39mheaders, req\u001b[39m.\u001b[39mget_method())\n\u001b[0;32m--> 519\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_open(req, data)\n\u001b[1;32m    521\u001b[0m \u001b[39m# post-process response\u001b[39;00m\n\u001b[1;32m    522\u001b[0m meth_name \u001b[39m=\u001b[39m protocol\u001b[39m+\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m_response\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/urllib/request.py:536\u001b[0m, in \u001b[0;36mOpenerDirector._open\u001b[0;34m(self, req, data)\u001b[0m\n\u001b[1;32m    533\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n\u001b[1;32m    535\u001b[0m protocol \u001b[39m=\u001b[39m req\u001b[39m.\u001b[39mtype\n\u001b[0;32m--> 536\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_chain(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle_open, protocol, protocol \u001b[39m+\u001b[39m\n\u001b[1;32m    537\u001b[0m                           \u001b[39m'\u001b[39m\u001b[39m_open\u001b[39m\u001b[39m'\u001b[39m, req)\n\u001b[1;32m    538\u001b[0m \u001b[39mif\u001b[39;00m result:\n\u001b[1;32m    539\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/urllib/request.py:496\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    494\u001b[0m \u001b[39mfor\u001b[39;00m handler \u001b[39min\u001b[39;00m handlers:\n\u001b[1;32m    495\u001b[0m     func \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(handler, meth_name)\n\u001b[0;32m--> 496\u001b[0m     result \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39margs)\n\u001b[1;32m    497\u001b[0m     \u001b[39mif\u001b[39;00m result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    498\u001b[0m         \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/urllib/request.py:1391\u001b[0m, in \u001b[0;36mHTTPSHandler.https_open\u001b[0;34m(self, req)\u001b[0m\n\u001b[1;32m   1390\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mhttps_open\u001b[39m(\u001b[39mself\u001b[39m, req):\n\u001b[0;32m-> 1391\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdo_open(http\u001b[39m.\u001b[39mclient\u001b[39m.\u001b[39mHTTPSConnection, req,\n\u001b[1;32m   1392\u001b[0m         context\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_context, check_hostname\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_hostname)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/urllib/request.py:1348\u001b[0m, in \u001b[0;36mAbstractHTTPHandler.do_open\u001b[0;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[1;32m   1346\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1347\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1348\u001b[0m         h\u001b[39m.\u001b[39mrequest(req\u001b[39m.\u001b[39mget_method(), req\u001b[39m.\u001b[39mselector, req\u001b[39m.\u001b[39mdata, headers,\n\u001b[1;32m   1349\u001b[0m                   encode_chunked\u001b[39m=\u001b[39mreq\u001b[39m.\u001b[39mhas_header(\u001b[39m'\u001b[39m\u001b[39mTransfer-encoding\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[1;32m   1350\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m \u001b[39mas\u001b[39;00m err: \u001b[39m# timeout error\u001b[39;00m\n\u001b[1;32m   1351\u001b[0m         \u001b[39mraise\u001b[39;00m URLError(err)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/http/client.py:1286\u001b[0m, in \u001b[0;36mHTTPConnection.request\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1283\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\u001b[39mself\u001b[39m, method, url, body\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, headers\u001b[39m=\u001b[39m{}, \u001b[39m*\u001b[39m,\n\u001b[1;32m   1284\u001b[0m             encode_chunked\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m   1285\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Send a complete request to the server.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1286\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_send_request(method, url, body, headers, encode_chunked)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/http/client.py:1297\u001b[0m, in \u001b[0;36mHTTPConnection._send_request\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1294\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39maccept-encoding\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m header_names:\n\u001b[1;32m   1295\u001b[0m     skips[\u001b[39m'\u001b[39m\u001b[39mskip_accept_encoding\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m-> 1297\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mputrequest(method, url, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mskips)\n\u001b[1;32m   1299\u001b[0m \u001b[39m# chunked encoding will happen if HTTP/1.1 is used and either\u001b[39;00m\n\u001b[1;32m   1300\u001b[0m \u001b[39m# the caller passes encode_chunked=True or the following\u001b[39;00m\n\u001b[1;32m   1301\u001b[0m \u001b[39m# conditions hold:\u001b[39;00m\n\u001b[1;32m   1302\u001b[0m \u001b[39m# 1. content-length has not been explicitly set\u001b[39;00m\n\u001b[1;32m   1303\u001b[0m \u001b[39m# 2. the body is a file or iterable, but not a str or bytes-like\u001b[39;00m\n\u001b[1;32m   1304\u001b[0m \u001b[39m# 3. Transfer-Encoding has NOT been explicitly set by the caller\u001b[39;00m\n\u001b[1;32m   1306\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mcontent-length\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m header_names:\n\u001b[1;32m   1307\u001b[0m     \u001b[39m# only chunk body if not explicitly set for backwards\u001b[39;00m\n\u001b[1;32m   1308\u001b[0m     \u001b[39m# compatibility, assuming the client code is already handling the\u001b[39;00m\n\u001b[1;32m   1309\u001b[0m     \u001b[39m# chunking\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/http/client.py:1135\u001b[0m, in \u001b[0;36mHTTPConnection.putrequest\u001b[0;34m(self, method, url, skip_host, skip_accept_encoding)\u001b[0m\n\u001b[1;32m   1131\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_path(url)\n\u001b[1;32m   1133\u001b[0m request \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m (method, url, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_http_vsn_str)\n\u001b[0;32m-> 1135\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_encode_request(request))\n\u001b[1;32m   1137\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_http_vsn \u001b[39m==\u001b[39m \u001b[39m11\u001b[39m:\n\u001b[1;32m   1138\u001b[0m     \u001b[39m# Issue some standard headers for better HTTP/1.1 compliance\u001b[39;00m\n\u001b[1;32m   1140\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m skip_host:\n\u001b[1;32m   1141\u001b[0m         \u001b[39m# this header is issued *only* for HTTP/1.1\u001b[39;00m\n\u001b[1;32m   1142\u001b[0m         \u001b[39m# connections. more specifically, this means it is\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1152\u001b[0m         \u001b[39m# but the host of the actual URL, not the host of the\u001b[39;00m\n\u001b[1;32m   1153\u001b[0m         \u001b[39m# proxy.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/http/client.py:1215\u001b[0m, in \u001b[0;36mHTTPConnection._encode_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m   1213\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_encode_request\u001b[39m(\u001b[39mself\u001b[39m, request):\n\u001b[1;32m   1214\u001b[0m     \u001b[39m# ASCII also helps prevent CVE-2019-9740.\u001b[39;00m\n\u001b[0;32m-> 1215\u001b[0m     \u001b[39mreturn\u001b[39;00m request\u001b[39m.\u001b[39mencode(\u001b[39m'\u001b[39m\u001b[39mascii\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mUnicodeEncodeError\u001b[0m: 'ascii' codec can't encode character '\\xe9' in position 19: ordinal not in range(128)"
     ]
    }
   ],
   "source": [
    "for alias, content in alias_to_content.items():\n",
    "    print(\"Fetching for \" + alias)\n",
    "    \n",
    "    # If we already scraped reviews, do not do it again.\n",
    "    # Scrapes take a long time, so this allows us to scrape over multiple runs.\n",
    "    if \"reviews\" in content:\n",
    "        continue\n",
    "    \n",
    "    # Gather 10 reviews per restaurant 5 times.\n",
    "    for i in range(0, 50, 10):\n",
    "        url = \"https://www.yelp.com/biz/{}/props?start={}\".format(alias, i)\n",
    "        request = urllib.request.Request(url, headers=headers)\n",
    "        \n",
    "        # Sleep or else Yelp might block your IP.\n",
    "        time.sleep(2)\n",
    "        \n",
    "        with urllib.request.urlopen(request) as response:\n",
    "            if response.code != 200:\n",
    "                print(response.code)\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                reviews = json.loads(response.read())[\"bizDetailsPageProps\"][\"reviewFeedQueryProps\"][\"reviews\"]\n",
    "                content[\"reviews\"] = content.get(\"reviews\", [])\n",
    "                \n",
    "                for r in reviews:\n",
    "                    content[\"reviews\"].append({\n",
    "                        \"photoCount\": r[\"user\"][\"photoCount\"],\n",
    "                        \"reviewCount\": r[\"user\"][\"reviewCount\"],\n",
    "                        \"eliteYear\": r[\"user\"][\"eliteYear\"],\n",
    "                        \"localizedDate\": r[\"localizedDate\"],\n",
    "                        \"comment\": r[\"comment\"],\n",
    "                        \"rating\": r[\"rating\"],\n",
    "                    })\n",
    "                    \n",
    "            except:\n",
    "                continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "9139790d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-13T03:52:52.348961Z",
     "start_time": "2023-11-13T03:52:51.789794Z"
    }
   },
   "outputs": [],
   "source": [
    "# Write file.\n",
    "#file_path = \"{}/restaurants.json\".format(os.getcwd())\n",
    "#with open(file_path, 'w') as fp:\n",
    "#    json.dump(alias_to_content, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aa5eec4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy\n",
    "file_path = \"{}/restaurants2.json\".format(os.getcwd())\n",
    "with open(file_path, 'w') as fp:\n",
    "    json.dump(alias_to_content, fp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
